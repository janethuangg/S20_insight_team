{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import stuff (a lot of stuff) & read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "import datetime as dt\n",
    "pd.options.mode.chained_assignment = None\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns= 1000\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn import metrics\n",
    "import scipy.stats as stats\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../S20_insight_team/datasets/ks-projects-201801.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data cleaning / transforming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a new column denoting length of project name (number of characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['titleLength'] = data['name'].apply(lambda x: len(str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop columns that won't be used at the moment (main_category, ID, name, state, pledged, usd pledged, goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['main_category','ID','name','state','pledged','usd pledged','goal'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "format date columns properly and get rid of records with invalid dates (precede the year 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['launched'] = pd.to_datetime(data['launched'])\n",
    "data['deadline'] = pd.to_datetime(data['deadline'])\n",
    "df = data[data['launched']>'2000-01-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eliminate projects with usd pledged real > 20000 or usd pledged real = 0\n",
    "\n",
    "TODO: write a brief explanation as to why you are doing this (double-click on this cell to edit the text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[(data['usd_pledged_real']<20000) & (data['usd_pledged_real']>0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create logged versions of usd pledged real and usd goal real + drop the original nonlogged versions\n",
    "\n",
    "TODO: write a brief explanation as to why you are doing this\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['logPledged'] = np.log(df['usd_pledged_real'])\n",
    "df['logGoal'] = np.log(df['usd_goal_real'])\n",
    "\n",
    "df.drop(columns=['usd_goal_real','usd_pledged_real'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create columns to encode the 'launched' column -- I chose to break it down into month, day of week, and hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['launchMonth'] = df['launched'].dt.month\n",
    "df['launchDay'] = df['launched'].dt.dayofweek\n",
    "df['launchHour'] = df['launched'].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate the duration of each project campaign + drop the launched & deadline columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['duration'] = (df['deadline']-df['launched'])/dt.timedelta(minutes=1)\n",
    "df.drop(columns=['deadline','launched'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "divide dataset into features (X) and target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['logPledged'])\n",
    "y = df['logPledged']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare the pipeline (for help, see bottom of this article https://kiwidamien.github.io/encoding-categorical-variables.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some helper code for you\n",
    "import joblib\n",
    "location = 'cache'\n",
    "memory = joblib.Memory(location=location, verbose=10)\n",
    "\n",
    "encoding_pipeline = Pipeline([\n",
    "    ('encode_category', ce.HashingEncoder(cols=['category'], return_df=True)),\n",
    "    ('encode_other', ce.OneHotEncoder(cols=['currency','country','launchMonth','launchDay','launchHour'], return_df=True)),\n",
    "], memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train-test-split your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encode your training set and test set (for reference, see documentation here https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)\n",
    "\n",
    "TODO: explain when to use fit_transform( ) vs transform( ) and why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = encoding_pipeline.fit_transform(X_train, y_train)\n",
    "X_test_encoded = encoding_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check to make sure that the dimensions align between test & training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_encoded.shape)\n",
    "print(X_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create & fit linear regression model on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train_encoded,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate & output MAE, MSE, RMSE based on test set AND interpret them in the context of this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = metrics.mean_absolute_error(y_test,linreg.predict(X_test_encoded))\n",
    "mse = metrics.mean_squared_error(y_test,linreg.predict(X_test_encoded))\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test,linreg.predict(X_test_encoded)))\n",
    "r2 = metrics.r2_score(y_test, linreg.predict(X_test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mae: \", mae)\n",
    "print(\"mse: \", mse)\n",
    "print(\"rmse: \", rmse)\n",
    "print(\"r2: \", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTR / RF\n",
    "I'm assuming you get the hang of things by now -- just repeat the process\n",
    "\n",
    "*don't worry about using GridSearch at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(X_train_encoded,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = metrics.mean_absolute_error(y_test,dtr.predict(X_test_encoded))\n",
    "mse = metrics.mean_squared_error(y_test,dtr.predict(X_test_encoded))\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test,dtr.predict(X_test_encoded)))\n",
    "r2 = metrics.r2_score(y_test, dtr.predict(X_test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"mae: \", mae)\n",
    "print(\"mse: \", mse)\n",
    "print(\"rmse: \", rmse)\n",
    "print(\"r2: \", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(X_train_encoded,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = metrics.mean_absolute_error(y_test,rfr.predict(X_test_encoded))\n",
    "mse = metrics.mean_squared_error(y_test,rfr.predict(X_test_encoded))\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test,rfr.predict(X_test_encoded)))\n",
    "r2 = metrics.r2_score(y_test, rfr.predict(X_test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"mae: \", mae)\n",
    "print(\"mse: \", mse)\n",
    "print(\"rmse: \", rmse)\n",
    "print(\"r2: \", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature importances\n",
    "pd.DataFrame({'feature':X_test_encoded.columns, \n",
    "              'importance':rfr.feature_importances_}).sort_values(by='importance',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using gridsearch with pipelines,  creating a customn scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no need to TTS when using GridSearch\n",
    "X = data.drop(columns=['logPledged'])\n",
    "y = data['logPledged']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaler\n",
    "- standardize features by removing the mean and scaling to unit variance\n",
    "- calculated as: z = (x - u) / s\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- as you can see, the basic implementation doesn't really work here since it can only apply the scaler to the entire dataset, which contains categorical columns\n",
    "- thus, we are going to wrap it into a function that allows us to apply the scaler to specific functions\n",
    "\n",
    "### ColumnTransformer\n",
    "- applies transformers to columns of an array or dataframe\n",
    "- allows different columns to be transformed separately, which is useful for heterogeneous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#why we need to wrap this into a function\n",
    "scaler = ColumnTransformer([\n",
    "            ('standardize', StandardScaler(), ['duration','titleLength'])\n",
    "        ])\n",
    "\n",
    "scaler.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you can think of 'standardize' as a step that applies StandardScaler() to the duration and titleLength columns\n",
    "- unfortunately, this returns a single array corresponding to the newly-scaled columns, but we want it to result in an entire dataframe containing the newly-scaled columns\n",
    "- why is this? check out the output of running each step in your original pipeline -- they're all dataframes, so we want to keep things consistent\n",
    "- thus, we are going to wrap everything into yet another function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling(X,y):\n",
    "    scaler = ColumnTransformer([\n",
    "            ('standardize', StandardScaler(), ['duration','titleLength'])\n",
    "        ])\n",
    "    X[['duration','titleLength']] = scaler.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this function, we...\n",
    "- pass in the feature and target dataframes\n",
    "- create a \"scaler\" object as described earlier\n",
    "- fit the scaler & transform the data to get the newly-scaled columns (in the form of an array) and then replace the original unscaled columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "- the first two steps are the same as before\n",
    "- we've added the additional step 'scale' that runs the scaling function we defined above\n",
    "- in order to run gridsearch, we also need another step that refers to the model\n",
    "- in this case, the step is called 'forest', and it runs the random forest regressor defined earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('hash', ce.HashingEncoder(cols=['category'])),\n",
    "    ('onehot', ce.OneHotEncoder(cols=['currency','country','launchMonth','launchDay','launchHour'])),\n",
    "    ('scale', scaling(X,y)),\n",
    "    ('forest', rfr)\n",
    "], memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch\n",
    "- as you may know, gridsearch allows you to try different values and combinations of parameters\n",
    "- thus, we need to create a grid of paramters to test (in the form of a dictionary)\n",
    "- the syntax of each dictionary entry is 'pipelineModelStepName__parameterName' : [values to test]\n",
    "- you can find the list of possible parameterNames in the documentation for DT/RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'forest__max_depth':[10,20],\n",
    "              'forest__min_samples_leaf':[25,50]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipeline,parameters)\n",
    "\n",
    "gs.fit(X_train,y_train)\n",
    "\n",
    "scores = pd.DataFrame(gs.cv_results_).filter(regex='param_+|mean_test_score'\n",
    "                                            ).sort_values('mean_test_score',\n",
    "                                                          ascending=False).reset_index().drop(['index'],axis=1)\n",
    "scores.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
