{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import stuff (a lot of stuff) & read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "import datetime as dt\n",
    "pd.options.mode.chained_assignment = None\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns= 1000\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn import metrics\n",
    "import scipy.stats as stats\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../datasets/ks-projects-201801.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data cleaning / transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['titleLength'] = df['name'].apply(lambda x: len(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['category','backers','ID','name','state','pledged','usd pledged','goal'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main_category</th>\n",
       "      <th>currency</th>\n",
       "      <th>deadline</th>\n",
       "      <th>launched</th>\n",
       "      <th>country</th>\n",
       "      <th>usd_pledged_real</th>\n",
       "      <th>usd_goal_real</th>\n",
       "      <th>titleLength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Publishing</td>\n",
       "      <td>GBP</td>\n",
       "      <td>2015-10-09</td>\n",
       "      <td>2015-08-11 12:12:28</td>\n",
       "      <td>GB</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1533.95</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2017-11-01</td>\n",
       "      <td>2017-09-02 04:43:57</td>\n",
       "      <td>US</td>\n",
       "      <td>2421.0</td>\n",
       "      <td>30000.00</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2013-02-26</td>\n",
       "      <td>2013-01-12 00:20:50</td>\n",
       "      <td>US</td>\n",
       "      <td>220.0</td>\n",
       "      <td>45000.00</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Music</td>\n",
       "      <td>USD</td>\n",
       "      <td>2012-04-16</td>\n",
       "      <td>2012-03-17 03:24:11</td>\n",
       "      <td>US</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2015-08-29</td>\n",
       "      <td>2015-07-04 08:35:03</td>\n",
       "      <td>US</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>19500.00</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378656</th>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2014-10-17</td>\n",
       "      <td>2014-09-17 02:35:30</td>\n",
       "      <td>US</td>\n",
       "      <td>25.0</td>\n",
       "      <td>50000.00</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378657</th>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2011-07-19</td>\n",
       "      <td>2011-06-22 03:35:14</td>\n",
       "      <td>US</td>\n",
       "      <td>155.0</td>\n",
       "      <td>1500.00</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378658</th>\n",
       "      <td>Film &amp; Video</td>\n",
       "      <td>USD</td>\n",
       "      <td>2010-08-16</td>\n",
       "      <td>2010-07-01 19:40:30</td>\n",
       "      <td>US</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15000.00</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378659</th>\n",
       "      <td>Technology</td>\n",
       "      <td>USD</td>\n",
       "      <td>2016-02-13</td>\n",
       "      <td>2016-01-13 18:13:53</td>\n",
       "      <td>US</td>\n",
       "      <td>200.0</td>\n",
       "      <td>15000.00</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378660</th>\n",
       "      <td>Art</td>\n",
       "      <td>USD</td>\n",
       "      <td>2011-08-16</td>\n",
       "      <td>2011-07-19 09:07:47</td>\n",
       "      <td>US</td>\n",
       "      <td>524.0</td>\n",
       "      <td>2000.00</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>378661 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       main_category currency    deadline             launched country  \\\n",
       "0         Publishing      GBP  2015-10-09  2015-08-11 12:12:28      GB   \n",
       "1       Film & Video      USD  2017-11-01  2017-09-02 04:43:57      US   \n",
       "2       Film & Video      USD  2013-02-26  2013-01-12 00:20:50      US   \n",
       "3              Music      USD  2012-04-16  2012-03-17 03:24:11      US   \n",
       "4       Film & Video      USD  2015-08-29  2015-07-04 08:35:03      US   \n",
       "...              ...      ...         ...                  ...     ...   \n",
       "378656  Film & Video      USD  2014-10-17  2014-09-17 02:35:30      US   \n",
       "378657  Film & Video      USD  2011-07-19  2011-06-22 03:35:14      US   \n",
       "378658  Film & Video      USD  2010-08-16  2010-07-01 19:40:30      US   \n",
       "378659    Technology      USD  2016-02-13  2016-01-13 18:13:53      US   \n",
       "378660           Art      USD  2011-08-16  2011-07-19 09:07:47      US   \n",
       "\n",
       "        usd_pledged_real  usd_goal_real  titleLength  \n",
       "0                    0.0        1533.95           31  \n",
       "1                 2421.0       30000.00           45  \n",
       "2                  220.0       45000.00           14  \n",
       "3                    1.0        5000.00           49  \n",
       "4                 1283.0       19500.00           58  \n",
       "...                  ...            ...          ...  \n",
       "378656              25.0       50000.00           49  \n",
       "378657             155.0        1500.00            9  \n",
       "378658              20.0       15000.00           71  \n",
       "378659             200.0       15000.00           24  \n",
       "378660             524.0        2000.00           33  \n",
       "\n",
       "[378661 rows x 8 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['launched'] = pd.to_datetime(df['launched'])\n",
    "df['deadline'] = pd.to_datetime(df['deadline'])\n",
    "df = df[df['launched']>'2000-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[(data['usd_pledged_real']<20000) & (data['usd_pledged_real']>0)]\n",
    "df = df[(df['usd_goal_real']<20000) & (df['usd_goal_real']>0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['logPledged'] = np.log(df['usd_pledged_real'])\n",
    "df['logGoal'] = np.log(df['usd_goal_real'])\n",
    "\n",
    "df.drop(columns=['usd_goal_real','usd_pledged_real'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4ad5337d07b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'launchMonth'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'launched'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'launchDay'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'launched'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdayofweek\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'launchHour'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'launched'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5268\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5269\u001b[0m         ):\n\u001b[0;32m-> 5270\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5271\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0maccessor_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m# http://www.pydanny.com/cached-property.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/accessors.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mDatetimeProperties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can only use .dt accessor with datetimelike values\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "df['launchMonth'] = df['launched'].dt.month\n",
    "df['launchDay'] = df['launched'].dt.dayofweek\n",
    "df['launchHour'] = df['launched'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['duration'] = (df['deadline']-df['launched'])/dt.timedelta(minutes=1)\n",
    "df.drop(columns=['deadline','launched'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['logPledged'])\n",
    "y = df['logPledged']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).drop(['launchMonth','launchDay','launchHour'],axis=1).columns\n",
    "categorical_features = list(X.select_dtypes(include=['object']).columns) + ['launchMonth','launchDay','launchHour']\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', ce.OneHotEncoder(), categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "[CV] regressor__max_depth=5, regressor__min_samples_leaf=10 ..........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=5, regressor__min_samples_leaf=10, total=   8.8s\n",
      "[CV] regressor__max_depth=5, regressor__min_samples_leaf=10 ..........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.8s remaining:    0.0s\n",
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=5, regressor__min_samples_leaf=10, total=  15.0s\n",
      "[CV] regressor__max_depth=5, regressor__min_samples_leaf=10 ..........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=5, regressor__min_samples_leaf=10, total=   9.3s\n",
      "[CV] regressor__max_depth=5, regressor__min_samples_leaf=50 ..........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=5, regressor__min_samples_leaf=50, total=   9.9s\n",
      "[CV] regressor__max_depth=5, regressor__min_samples_leaf=50 ..........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=5, regressor__min_samples_leaf=50, total=  10.0s\n",
      "[CV] regressor__max_depth=5, regressor__min_samples_leaf=50 ..........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=5, regressor__min_samples_leaf=50, total=  10.3s\n",
      "[CV] regressor__max_depth=5, regressor__min_samples_leaf=100 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=5, regressor__min_samples_leaf=100, total=   9.9s\n",
      "[CV] regressor__max_depth=5, regressor__min_samples_leaf=100 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=5, regressor__min_samples_leaf=100, total=  12.9s\n",
      "[CV] regressor__max_depth=5, regressor__min_samples_leaf=100 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=5, regressor__min_samples_leaf=100, total=  15.8s\n",
      "[CV] regressor__max_depth=10, regressor__min_samples_leaf=10 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=10, regressor__min_samples_leaf=10, total=  29.3s\n",
      "[CV] regressor__max_depth=10, regressor__min_samples_leaf=10 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=10, regressor__min_samples_leaf=10, total=  27.5s\n",
      "[CV] regressor__max_depth=10, regressor__min_samples_leaf=10 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=10, regressor__min_samples_leaf=10, total=  33.8s\n",
      "[CV] regressor__max_depth=10, regressor__min_samples_leaf=50 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=10, regressor__min_samples_leaf=50, total=  27.0s\n",
      "[CV] regressor__max_depth=10, regressor__min_samples_leaf=50 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=10, regressor__min_samples_leaf=50, total=  16.7s\n",
      "[CV] regressor__max_depth=10, regressor__min_samples_leaf=50 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=10, regressor__min_samples_leaf=50, total=  20.3s\n",
      "[CV] regressor__max_depth=10, regressor__min_samples_leaf=100 ........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=10, regressor__min_samples_leaf=100, total=  20.9s\n",
      "[CV] regressor__max_depth=10, regressor__min_samples_leaf=100 ........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=10, regressor__min_samples_leaf=100, total=  16.3s\n",
      "[CV] regressor__max_depth=10, regressor__min_samples_leaf=100 ........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=10, regressor__min_samples_leaf=100, total=  18.9s\n",
      "[CV] regressor__max_depth=20, regressor__min_samples_leaf=10 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=20, regressor__min_samples_leaf=10, total=  27.8s\n",
      "[CV] regressor__max_depth=20, regressor__min_samples_leaf=10 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=20, regressor__min_samples_leaf=10, total=  33.6s\n",
      "[CV] regressor__max_depth=20, regressor__min_samples_leaf=10 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=20, regressor__min_samples_leaf=10, total=  28.3s\n",
      "[CV] regressor__max_depth=20, regressor__min_samples_leaf=50 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=20, regressor__min_samples_leaf=50, total=  25.3s\n",
      "[CV] regressor__max_depth=20, regressor__min_samples_leaf=50 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=20, regressor__min_samples_leaf=50, total=  28.1s\n",
      "[CV] regressor__max_depth=20, regressor__min_samples_leaf=50 .........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=20, regressor__min_samples_leaf=50, total=  28.2s\n",
      "[CV] regressor__max_depth=20, regressor__min_samples_leaf=100 ........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=20, regressor__min_samples_leaf=100, total=  33.1s\n",
      "[CV] regressor__max_depth=20, regressor__min_samples_leaf=100 ........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=20, regressor__min_samples_leaf=100, total=  26.1s\n",
      "[CV] regressor__max_depth=20, regressor__min_samples_leaf=100 ........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=20, regressor__min_samples_leaf=100, total=  18.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:  9.4min finished\n",
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'regressor__max_depth': 20, 'regressor__min_samples_leaf': 100}\n",
      "0.1440855187293938\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('regressor', RandomForestRegressor())])\n",
    "param_grid = { \n",
    "    'regressor__max_depth' : [5,10,20],\n",
    "    'regressor__min_samples_leaf': [10,50,100]\n",
    "}\n",
    "\n",
    "CV = GridSearchCV(rf, param_grid, n_jobs=1,verbose=2)\n",
    "                  \n",
    "CV.fit(X, y)  \n",
    "print(CV.best_params_)    \n",
    "print(CV.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3        0.00000\n",
       "4        7.15696\n",
       "6        7.09423\n",
       "11       9.44936\n",
       "15       6.49828\n",
       "           ...  \n",
       "378654   5.03695\n",
       "378657   5.04343\n",
       "378658   2.99573\n",
       "378659   5.29832\n",
       "378660   6.26149\n",
       "Name: logPledged, Length: 245957, dtype: float64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
      "model score: 0.069\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=20, max_features=None,\n",
      "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                      min_impurity_split=None, min_samples_leaf=50,\n",
      "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                      presort=False, random_state=None, splitter='best')\n",
      "model score: 0.111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
      "                      max_features='auto', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=50, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=10,\n",
      "                      n_jobs=None, oob_score=False, random_state=None,\n",
      "                      verbose=0, warm_start=False)\n",
      "model score: 0.146\n",
      "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "             hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "             learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "             random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "model score: 0.143\n"
     ]
    }
   ],
   "source": [
    "regressors = [\n",
    "        LogisitcRegression()\n",
    "        DecisionTreeClassifier(max_depth=20, min_samples_leaf=50),\n",
    "        RandomForestClassifier(max_depth=20, min_samples_leaf=50),\n",
    "        MLPClassifier()\n",
    "    ]\n",
    "\n",
    "for regressor in regressors:\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('regressor', regressor)])\n",
    "    pipe.fit(X_train, y_train)   \n",
    "    print(regressor)\n",
    "    print(\"model score: %.3f\" % pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['titleLength', 'logGoal', 'duration'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['main_category',\n",
       " 'currency',\n",
       " 'country',\n",
       " 'launchMonth',\n",
       " 'launchDay',\n",
       " 'launchHour']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.96807231\n",
      "Iteration 2, loss = 2.56446634\n",
      "Iteration 3, loss = 2.54674861\n",
      "Iteration 4, loss = 2.53151186\n",
      "Iteration 5, loss = 2.52504591\n",
      "Iteration 6, loss = 2.51269126\n",
      "Iteration 7, loss = 2.50581863\n",
      "Iteration 8, loss = 2.50135791\n",
      "Iteration 9, loss = 2.49812243\n",
      "Iteration 10, loss = 2.49409654\n",
      "Iteration 11, loss = 2.49038793\n",
      "Iteration 12, loss = 2.48806069\n",
      "Iteration 13, loss = 2.48409364\n",
      "Iteration 14, loss = 2.48237889\n",
      "Iteration 15, loss = 2.48184141\n",
      "Iteration 16, loss = 2.47797948\n",
      "Iteration 17, loss = 2.47605688\n",
      "Iteration 18, loss = 2.47451411\n",
      "Iteration 19, loss = 2.47149246\n",
      "Iteration 20, loss = 2.47140902\n",
      "Iteration 21, loss = 2.47170700\n",
      "Iteration 22, loss = 2.46855527\n",
      "Iteration 23, loss = 2.46677962\n",
      "Iteration 24, loss = 2.46493043\n",
      "Iteration 25, loss = 2.46474464\n",
      "Iteration 26, loss = 2.46251633\n",
      "Iteration 27, loss = 2.46024923\n",
      "Iteration 28, loss = 2.45951591\n",
      "Iteration 29, loss = 2.45856873\n",
      "Iteration 30, loss = 2.45685183\n",
      "Iteration 31, loss = 2.45501231\n",
      "Iteration 32, loss = 2.45156506\n",
      "Iteration 33, loss = 2.44856840\n",
      "Iteration 34, loss = 2.44624991\n",
      "Iteration 35, loss = 2.44154549\n",
      "Iteration 36, loss = 2.43873576\n",
      "Iteration 37, loss = 2.43918611\n",
      "Iteration 38, loss = 2.43624907\n",
      "Iteration 39, loss = 2.43450149\n",
      "Iteration 40, loss = 2.43470067\n",
      "Iteration 41, loss = 2.42990269\n",
      "Iteration 42, loss = 2.43087808\n",
      "Iteration 43, loss = 2.42957643\n",
      "Iteration 44, loss = 2.42846942\n",
      "Iteration 45, loss = 2.42496582\n",
      "Iteration 46, loss = 2.42622638\n",
      "Iteration 47, loss = 2.42522791\n",
      "Iteration 48, loss = 2.42139015\n",
      "Iteration 49, loss = 2.42269510\n",
      "Iteration 50, loss = 2.42357799\n",
      "Iteration 51, loss = 2.42185017\n",
      "Iteration 52, loss = 2.42168207\n",
      "Iteration 53, loss = 2.42057918\n",
      "Iteration 54, loss = 2.41902762\n",
      "Iteration 55, loss = 2.41805758\n",
      "Iteration 56, loss = 2.41733733\n",
      "Iteration 57, loss = 2.41687516\n",
      "Iteration 58, loss = 2.41792468\n",
      "Iteration 59, loss = 2.41542075\n",
      "Iteration 60, loss = 2.41560280\n",
      "Iteration 61, loss = 2.41439010\n",
      "Iteration 62, loss = 2.41485827\n",
      "Iteration 63, loss = 2.41403150\n",
      "Iteration 64, loss = 2.41357782\n",
      "Iteration 65, loss = 2.41352354\n",
      "Iteration 66, loss = 2.41400397\n",
      "Iteration 67, loss = 2.41149400\n",
      "Iteration 68, loss = 2.41264320\n",
      "Iteration 69, loss = 2.41071394\n",
      "Iteration 70, loss = 2.41067902\n",
      "Iteration 71, loss = 2.41137913\n",
      "Iteration 72, loss = 2.40980920\n",
      "Iteration 73, loss = 2.41137448\n",
      "Iteration 74, loss = 2.40814213\n",
      "Iteration 75, loss = 2.40933261\n",
      "Iteration 76, loss = 2.40912875\n",
      "Iteration 77, loss = 2.40956552\n",
      "Iteration 78, loss = 2.40829448\n",
      "Iteration 79, loss = 2.40655886\n",
      "Iteration 80, loss = 2.40705814\n",
      "Iteration 81, loss = 2.40738231\n",
      "Iteration 82, loss = 2.40602453\n",
      "Iteration 83, loss = 2.40575184\n",
      "Iteration 84, loss = 2.40613449\n",
      "Iteration 85, loss = 2.40461448\n",
      "Iteration 86, loss = 2.40643808\n",
      "Iteration 87, loss = 2.40474517\n",
      "Iteration 88, loss = 2.40397045\n",
      "Iteration 89, loss = 2.40414606\n",
      "Iteration 90, loss = 2.40386121\n",
      "Iteration 91, loss = 2.40298026\n",
      "Iteration 92, loss = 2.40096909\n",
      "Iteration 93, loss = 2.40326015\n",
      "Iteration 94, loss = 2.40042556\n",
      "Iteration 95, loss = 2.40248595\n",
      "Iteration 96, loss = 2.40086503\n",
      "Iteration 97, loss = 2.40135988\n",
      "Iteration 98, loss = 2.40136842\n",
      "Iteration 99, loss = 2.40054076\n",
      "Iteration 100, loss = 2.39871156\n",
      "Iteration 101, loss = 2.40029432\n",
      "Iteration 102, loss = 2.39927793\n",
      "Iteration 103, loss = 2.39944913\n",
      "Iteration 104, loss = 2.39852749\n",
      "Iteration 105, loss = 2.39872377\n",
      "Iteration 106, loss = 2.39837175\n",
      "Iteration 107, loss = 2.39842605\n",
      "Iteration 108, loss = 2.39776954\n",
      "Iteration 109, loss = 2.39699936\n",
      "Iteration 110, loss = 2.39702985\n",
      "Iteration 111, loss = 2.39748623\n",
      "Iteration 112, loss = 2.39770807\n",
      "Iteration 113, loss = 2.39528251\n",
      "Iteration 114, loss = 2.39798258\n",
      "Iteration 115, loss = 2.39442924\n",
      "Iteration 116, loss = 2.39617284\n",
      "Iteration 117, loss = 2.39776742\n",
      "Iteration 118, loss = 2.39505188\n",
      "Iteration 119, loss = 2.39595563\n",
      "Iteration 120, loss = 2.39568316\n",
      "Iteration 121, loss = 2.39564499\n",
      "Iteration 122, loss = 2.39396907\n",
      "Iteration 123, loss = 2.39382001\n",
      "Iteration 124, loss = 2.39361255\n",
      "Iteration 125, loss = 2.39436903\n",
      "Iteration 126, loss = 2.39453705\n",
      "Iteration 127, loss = 2.39394655\n",
      "Iteration 128, loss = 2.39479454\n",
      "Iteration 129, loss = 2.39268730\n",
      "Iteration 130, loss = 2.39244384\n",
      "Iteration 131, loss = 2.39266635\n",
      "Iteration 132, loss = 2.39241568\n",
      "Iteration 133, loss = 2.39287554\n",
      "Iteration 134, loss = 2.39310677\n",
      "Iteration 135, loss = 2.39253795\n",
      "Iteration 136, loss = 2.39216433\n",
      "Iteration 137, loss = 2.39185770\n",
      "Iteration 138, loss = 2.39150695\n",
      "Iteration 139, loss = 2.39280588\n",
      "Iteration 140, loss = 2.39120190\n",
      "Iteration 141, loss = 2.39148204\n",
      "Iteration 142, loss = 2.39195218\n",
      "Iteration 143, loss = 2.39086491\n",
      "Iteration 144, loss = 2.39254495\n",
      "Iteration 145, loss = 2.39013696\n",
      "Iteration 146, loss = 2.38991497\n",
      "Iteration 147, loss = 2.39172903\n",
      "Iteration 148, loss = 2.39008814\n",
      "Iteration 149, loss = 2.39107772\n",
      "Iteration 150, loss = 2.38852038\n",
      "Iteration 151, loss = 2.38888106\n",
      "Iteration 152, loss = 2.38917921\n",
      "Iteration 153, loss = 2.38864072\n",
      "Iteration 154, loss = 2.38906584\n",
      "Iteration 155, loss = 2.38779963\n",
      "Iteration 156, loss = 2.38937387\n",
      "Iteration 157, loss = 2.38931125\n",
      "Iteration 158, loss = 2.38793528\n",
      "Iteration 159, loss = 2.38864611\n",
      "Iteration 160, loss = 2.38864098\n",
      "Iteration 161, loss = 2.38816962\n",
      "Iteration 162, loss = 2.38871544\n",
      "Iteration 163, loss = 2.38793776\n",
      "Iteration 164, loss = 2.38678966\n",
      "Iteration 165, loss = 2.38784311\n",
      "Iteration 166, loss = 2.38783439\n",
      "Iteration 167, loss = 2.38870269\n",
      "Iteration 168, loss = 2.38766750\n",
      "Iteration 169, loss = 2.38797930\n",
      "Iteration 170, loss = 2.38694871\n",
      "Iteration 171, loss = 2.38592553\n",
      "Iteration 172, loss = 2.38512083\n",
      "Iteration 173, loss = 2.38629538\n",
      "Iteration 174, loss = 2.38585990\n",
      "Iteration 175, loss = 2.38549569\n",
      "Iteration 176, loss = 2.38577541\n",
      "Iteration 177, loss = 2.38741045\n",
      "Iteration 178, loss = 2.38526801\n",
      "Iteration 179, loss = 2.38533953\n",
      "Iteration 180, loss = 2.38539885\n",
      "Iteration 181, loss = 2.38492752\n",
      "Iteration 182, loss = 2.38627732\n",
      "Iteration 183, loss = 2.38516708\n",
      "Iteration 184, loss = 2.38348339\n",
      "Iteration 185, loss = 2.38284015\n",
      "Iteration 186, loss = 2.38458693\n",
      "Iteration 187, loss = 2.38368351\n",
      "Iteration 188, loss = 2.38536472\n",
      "Iteration 189, loss = 2.38454606\n",
      "Iteration 190, loss = 2.38476026\n",
      "Iteration 191, loss = 2.38414349\n",
      "Iteration 192, loss = 2.38453332\n",
      "Iteration 193, loss = 2.38361239\n",
      "Iteration 194, loss = 2.38346186\n",
      "Iteration 195, loss = 2.38294253\n",
      "Iteration 196, loss = 2.38378175\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "             hidden_layer_sizes=(50, 10, 50), learning_rate='constant',\n",
      "             learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "             random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=True, warm_start=False)\n",
      "model score: 0.146\n",
      "Iteration 1, loss = 2.95026548\n",
      "Iteration 2, loss = 2.57163774\n",
      "Iteration 3, loss = 2.56576911\n",
      "Iteration 4, loss = 2.55596100\n",
      "Iteration 5, loss = 2.54467952\n",
      "Iteration 6, loss = 2.53709205\n",
      "Iteration 7, loss = 2.52941092\n",
      "Iteration 8, loss = 2.52761044\n",
      "Iteration 9, loss = 2.52005805\n",
      "Iteration 10, loss = 2.51924160\n",
      "Iteration 11, loss = 2.51451824\n",
      "Iteration 12, loss = 2.51224505\n",
      "Iteration 13, loss = 2.50727781\n",
      "Iteration 14, loss = 2.50571014\n",
      "Iteration 15, loss = 2.50398938\n",
      "Iteration 16, loss = 2.50141047\n",
      "Iteration 17, loss = 2.50009845\n",
      "Iteration 18, loss = 2.49785589\n",
      "Iteration 19, loss = 2.49601547\n",
      "Iteration 20, loss = 2.49313711\n",
      "Iteration 21, loss = 2.49184711\n",
      "Iteration 22, loss = 2.49156845\n",
      "Iteration 23, loss = 2.49013555\n",
      "Iteration 24, loss = 2.48792704\n",
      "Iteration 25, loss = 2.48790365\n",
      "Iteration 26, loss = 2.48487515\n",
      "Iteration 27, loss = 2.48410222\n",
      "Iteration 28, loss = 2.48280604\n",
      "Iteration 29, loss = 2.48200079\n",
      "Iteration 30, loss = 2.48200602\n",
      "Iteration 31, loss = 2.48107536\n",
      "Iteration 32, loss = 2.48037863\n",
      "Iteration 33, loss = 2.48004285\n",
      "Iteration 34, loss = 2.47567590\n",
      "Iteration 35, loss = 2.47872714\n",
      "Iteration 36, loss = 2.47585052\n",
      "Iteration 37, loss = 2.47439609\n",
      "Iteration 38, loss = 2.47392841\n",
      "Iteration 39, loss = 2.47360670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 40, loss = 2.47285577\n",
      "Iteration 41, loss = 2.47085144\n",
      "Iteration 42, loss = 2.46874094\n",
      "Iteration 43, loss = 2.47078063\n",
      "Iteration 44, loss = 2.46760094\n",
      "Iteration 45, loss = 2.46758327\n",
      "Iteration 46, loss = 2.46549562\n",
      "Iteration 47, loss = 2.46384665\n",
      "Iteration 48, loss = 2.46323882\n",
      "Iteration 49, loss = 2.46243582\n",
      "Iteration 50, loss = 2.46113888\n",
      "Iteration 51, loss = 2.45971028\n",
      "Iteration 52, loss = 2.45825170\n",
      "Iteration 53, loss = 2.45689490\n",
      "Iteration 54, loss = 2.45752918\n",
      "Iteration 55, loss = 2.45628993\n",
      "Iteration 56, loss = 2.45527935\n",
      "Iteration 57, loss = 2.45412296\n",
      "Iteration 58, loss = 2.45367511\n",
      "Iteration 59, loss = 2.45169243\n",
      "Iteration 60, loss = 2.45024347\n",
      "Iteration 61, loss = 2.44779608\n",
      "Iteration 62, loss = 2.44829101\n",
      "Iteration 63, loss = 2.44838606\n",
      "Iteration 64, loss = 2.44637185\n",
      "Iteration 65, loss = 2.44669668\n",
      "Iteration 66, loss = 2.44338316\n",
      "Iteration 67, loss = 2.44328457\n",
      "Iteration 68, loss = 2.44212423\n",
      "Iteration 69, loss = 2.44246755\n",
      "Iteration 70, loss = 2.44030493\n",
      "Iteration 71, loss = 2.44015367\n",
      "Iteration 72, loss = 2.43990947\n",
      "Iteration 73, loss = 2.43812234\n",
      "Iteration 74, loss = 2.43872186\n",
      "Iteration 75, loss = 2.43703479\n",
      "Iteration 76, loss = 2.43767525\n",
      "Iteration 77, loss = 2.43656270\n",
      "Iteration 78, loss = 2.43693123\n",
      "Iteration 79, loss = 2.43691157\n",
      "Iteration 80, loss = 2.43471645\n",
      "Iteration 81, loss = 2.43503057\n",
      "Iteration 82, loss = 2.43397620\n",
      "Iteration 83, loss = 2.43363363\n",
      "Iteration 84, loss = 2.43288307\n",
      "Iteration 85, loss = 2.43220876\n",
      "Iteration 86, loss = 2.43401501\n",
      "Iteration 87, loss = 2.43164130\n",
      "Iteration 88, loss = 2.43255807\n",
      "Iteration 89, loss = 2.43261660\n",
      "Iteration 90, loss = 2.43040385\n",
      "Iteration 91, loss = 2.43115972\n",
      "Iteration 92, loss = 2.42951071\n",
      "Iteration 93, loss = 2.43060484\n",
      "Iteration 94, loss = 2.43150052\n",
      "Iteration 95, loss = 2.43075512\n",
      "Iteration 96, loss = 2.42968559\n",
      "Iteration 97, loss = 2.43055336\n",
      "Iteration 98, loss = 2.42918593\n",
      "Iteration 99, loss = 2.42748964\n",
      "Iteration 100, loss = 2.42820483\n",
      "Iteration 101, loss = 2.42787013\n",
      "Iteration 102, loss = 2.42834917\n",
      "Iteration 103, loss = 2.42796053\n",
      "Iteration 104, loss = 2.42592007\n",
      "Iteration 105, loss = 2.42732779\n",
      "Iteration 106, loss = 2.42693389\n",
      "Iteration 107, loss = 2.42729393\n",
      "Iteration 108, loss = 2.42555797\n",
      "Iteration 109, loss = 2.42538215\n",
      "Iteration 110, loss = 2.42543022\n",
      "Iteration 111, loss = 2.42676030\n",
      "Iteration 112, loss = 2.42486588\n",
      "Iteration 113, loss = 2.42483807\n",
      "Iteration 114, loss = 2.42336576\n",
      "Iteration 115, loss = 2.42525505\n",
      "Iteration 116, loss = 2.42479722\n",
      "Iteration 117, loss = 2.42421897\n",
      "Iteration 118, loss = 2.42471679\n",
      "Iteration 119, loss = 2.42422771\n",
      "Iteration 120, loss = 2.42615356\n",
      "Iteration 121, loss = 2.42279536\n",
      "Iteration 122, loss = 2.42416036\n",
      "Iteration 123, loss = 2.42369970\n",
      "Iteration 124, loss = 2.42270917\n",
      "Iteration 125, loss = 2.42438408\n",
      "Iteration 126, loss = 2.42261310\n",
      "Iteration 127, loss = 2.42279312\n",
      "Iteration 128, loss = 2.42324660\n",
      "Iteration 129, loss = 2.42168258\n",
      "Iteration 130, loss = 2.42171537\n",
      "Iteration 131, loss = 2.42134087\n",
      "Iteration 132, loss = 2.42261171\n",
      "Iteration 133, loss = 2.42254113\n",
      "Iteration 134, loss = 2.42088818\n",
      "Iteration 135, loss = 2.42252344\n",
      "Iteration 136, loss = 2.42078675\n",
      "Iteration 137, loss = 2.42161741\n",
      "Iteration 138, loss = 2.42042226\n",
      "Iteration 139, loss = 2.42058155\n",
      "Iteration 140, loss = 2.41961229\n",
      "Iteration 141, loss = 2.42000754\n",
      "Iteration 142, loss = 2.41939800\n",
      "Iteration 143, loss = 2.41986184\n",
      "Iteration 144, loss = 2.41999022\n",
      "Iteration 145, loss = 2.42025953\n",
      "Iteration 146, loss = 2.41978704\n",
      "Iteration 147, loss = 2.42007157\n",
      "Iteration 148, loss = 2.41883526\n",
      "Iteration 149, loss = 2.41966409\n",
      "Iteration 150, loss = 2.41893808\n",
      "Iteration 151, loss = 2.41880343\n",
      "Iteration 152, loss = 2.41780226\n",
      "Iteration 153, loss = 2.42068773\n",
      "Iteration 154, loss = 2.41932836\n",
      "Iteration 155, loss = 2.41967159\n",
      "Iteration 156, loss = 2.41898464\n",
      "Iteration 157, loss = 2.41863614\n",
      "Iteration 158, loss = 2.41823394\n",
      "Iteration 159, loss = 2.41677549\n",
      "Iteration 160, loss = 2.41822697\n",
      "Iteration 161, loss = 2.41800625\n",
      "Iteration 162, loss = 2.41834110\n",
      "Iteration 163, loss = 2.41808512\n",
      "Iteration 164, loss = 2.41744768\n",
      "Iteration 165, loss = 2.41849112\n",
      "Iteration 166, loss = 2.41831207\n",
      "Iteration 167, loss = 2.41702753\n",
      "Iteration 168, loss = 2.41753225\n",
      "Iteration 169, loss = 2.41628803\n",
      "Iteration 170, loss = 2.41782786\n",
      "Iteration 171, loss = 2.41694078\n",
      "Iteration 172, loss = 2.41768810\n",
      "Iteration 173, loss = 2.41753446\n",
      "Iteration 174, loss = 2.41682684\n",
      "Iteration 175, loss = 2.41627567\n",
      "Iteration 176, loss = 2.41657885\n",
      "Iteration 177, loss = 2.41735921\n",
      "Iteration 178, loss = 2.41769667\n",
      "Iteration 179, loss = 2.41608769\n",
      "Iteration 180, loss = 2.41630568\n",
      "Iteration 181, loss = 2.41586160\n",
      "Iteration 182, loss = 2.41614420\n",
      "Iteration 183, loss = 2.41690025\n",
      "Iteration 184, loss = 2.41536178\n",
      "Iteration 185, loss = 2.41674761\n",
      "Iteration 186, loss = 2.41623279\n",
      "Iteration 187, loss = 2.41538327\n",
      "Iteration 188, loss = 2.41617143\n",
      "Iteration 189, loss = 2.41667640\n",
      "Iteration 190, loss = 2.41645857\n",
      "Iteration 191, loss = 2.41656075\n",
      "Iteration 192, loss = 2.41507180\n",
      "Iteration 193, loss = 2.41587210\n",
      "Iteration 194, loss = 2.41579718\n",
      "Iteration 195, loss = 2.41549395\n",
      "Iteration 196, loss = 2.41531862\n",
      "Iteration 197, loss = 2.41567265\n",
      "Iteration 198, loss = 2.41431946\n",
      "Iteration 199, loss = 2.41586689\n",
      "Iteration 200, loss = 2.41411927\n",
      "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "             hidden_layer_sizes=(25, 10, 25), learning_rate='constant',\n",
      "             learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "             random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=True, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.148\n"
     ]
    }
   ],
   "source": [
    "regressors = [\n",
    "        MLPRegressor(hidden_layer_sizes=(50,10,50), verbose=True),\n",
    "        MLPRegressor(hidden_layer_sizes=(25,10,25), verbose=True)\n",
    "    ]\n",
    "\n",
    "for regressor in regressors:\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('regressor', regressor)])\n",
    "    pipe.fit(X_train, y_train)   \n",
    "    print(regressor)\n",
    "    print(\"model score: %.3f\" % pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing geo/currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).drop(['launchMonth','launchDay','launchHour'],axis=1).columns\n",
    "categorical_features = list(X.select_dtypes(include=['object']).drop(['currency'],axis=1)) + ['launchMonth','launchDay','launchHour']\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', ce.OneHotEncoder(), categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['main_category', 'country', 'launchMonth', 'launchDay', 'launchHour']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=20,\n",
      "                      max_features='auto', max_leaf_nodes=None,\n",
      "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                      min_samples_leaf=50, min_samples_split=2,\n",
      "                      min_weight_fraction_leaf=0.0, n_estimators=20,\n",
      "                      n_jobs=None, oob_score=False, random_state=None,\n",
      "                      verbose=0, warm_start=False)\n",
      "model score: 0.150\n",
      "Iteration 1, loss = 2.80873759\n",
      "Iteration 2, loss = 2.45623672\n",
      "Iteration 3, loss = 2.43627204\n",
      "Iteration 4, loss = 2.42530815\n",
      "Iteration 5, loss = 2.42091731\n",
      "Iteration 6, loss = 2.40867663\n",
      "Iteration 7, loss = 2.40531594\n",
      "Iteration 8, loss = 2.40372776\n",
      "Iteration 9, loss = 2.39997618\n",
      "Iteration 10, loss = 2.39708740\n",
      "Iteration 11, loss = 2.39461412\n",
      "Iteration 12, loss = 2.39218906\n",
      "Iteration 13, loss = 2.39374626\n",
      "Iteration 14, loss = 2.39000960\n",
      "Iteration 15, loss = 2.38742010\n",
      "Iteration 16, loss = 2.38634258\n",
      "Iteration 17, loss = 2.38368651\n",
      "Iteration 18, loss = 2.38379086\n",
      "Iteration 19, loss = 2.38027682\n",
      "Iteration 20, loss = 2.37838894\n",
      "Iteration 21, loss = 2.38170573\n",
      "Iteration 22, loss = 2.37793491\n",
      "Iteration 23, loss = 2.37809893\n",
      "Iteration 24, loss = 2.37490978\n",
      "Iteration 25, loss = 2.37285437\n",
      "Iteration 26, loss = 2.37400910\n",
      "Iteration 27, loss = 2.37441950\n",
      "Iteration 28, loss = 2.36954505\n",
      "Iteration 29, loss = 2.37014111\n",
      "Iteration 30, loss = 2.36921990\n",
      "Iteration 31, loss = 2.36866462\n",
      "Iteration 32, loss = 2.36693398\n",
      "Iteration 33, loss = 2.36625370\n",
      "Iteration 34, loss = 2.36660294\n",
      "Iteration 35, loss = 2.36558801\n",
      "Iteration 36, loss = 2.36289413\n",
      "Iteration 37, loss = 2.36341215\n",
      "Iteration 38, loss = 2.36210731\n",
      "Iteration 39, loss = 2.36166865\n",
      "Iteration 40, loss = 2.36048784\n",
      "Iteration 41, loss = 2.36099543\n",
      "Iteration 42, loss = 2.35833404\n",
      "Iteration 43, loss = 2.35832622\n",
      "Iteration 44, loss = 2.36114542\n",
      "Iteration 45, loss = 2.35762059\n",
      "Iteration 46, loss = 2.35527171\n",
      "Iteration 47, loss = 2.35588063\n",
      "Iteration 48, loss = 2.35572500\n",
      "Iteration 49, loss = 2.35530619\n",
      "Iteration 50, loss = 2.35378353\n",
      "Iteration 51, loss = 2.35454516\n",
      "Iteration 52, loss = 2.35221417\n",
      "Iteration 53, loss = 2.35084457\n",
      "Iteration 54, loss = 2.35235460\n",
      "Iteration 55, loss = 2.35172444\n",
      "Iteration 56, loss = 2.35065209\n",
      "Iteration 57, loss = 2.34973758\n",
      "Iteration 58, loss = 2.34942065\n",
      "Iteration 59, loss = 2.34864266\n",
      "Iteration 60, loss = 2.34730381\n",
      "Iteration 61, loss = 2.34785871\n",
      "Iteration 62, loss = 2.34775451\n",
      "Iteration 63, loss = 2.34612490\n",
      "Iteration 64, loss = 2.34544495\n",
      "Iteration 65, loss = 2.34673869\n",
      "Iteration 66, loss = 2.34527957\n",
      "Iteration 67, loss = 2.34481967\n",
      "Iteration 68, loss = 2.34417765\n",
      "Iteration 69, loss = 2.34385691\n",
      "Iteration 70, loss = 2.34211768\n",
      "Iteration 71, loss = 2.34426110\n",
      "Iteration 72, loss = 2.34220733\n",
      "Iteration 73, loss = 2.34253354\n",
      "Iteration 74, loss = 2.33993112\n",
      "Iteration 75, loss = 2.34059768\n",
      "Iteration 76, loss = 2.34088503\n",
      "Iteration 77, loss = 2.33943538\n",
      "Iteration 78, loss = 2.33922850\n",
      "Iteration 79, loss = 2.33818156\n",
      "Iteration 80, loss = 2.33888363\n",
      "Iteration 81, loss = 2.33679289\n",
      "Iteration 82, loss = 2.33778663\n",
      "Iteration 83, loss = 2.33622185\n",
      "Iteration 84, loss = 2.33515321\n",
      "Iteration 85, loss = 2.33383951\n",
      "Iteration 86, loss = 2.33215229\n",
      "Iteration 87, loss = 2.33117069\n",
      "Iteration 88, loss = 2.33007276\n",
      "Iteration 89, loss = 2.33043609\n",
      "Iteration 90, loss = 2.32959625\n",
      "Iteration 91, loss = 2.32919102\n",
      "Iteration 92, loss = 2.33156555\n",
      "Iteration 93, loss = 2.32740488\n",
      "Iteration 94, loss = 2.32772281\n",
      "Iteration 95, loss = 2.32536464\n",
      "Iteration 96, loss = 2.32673366\n",
      "Iteration 97, loss = 2.32618203\n",
      "Iteration 98, loss = 2.32593681\n",
      "Iteration 99, loss = 2.32404437\n",
      "Iteration 100, loss = 2.32407164\n",
      "Iteration 101, loss = 2.32278774\n",
      "Iteration 102, loss = 2.32444708\n",
      "Iteration 103, loss = 2.32202547\n",
      "Iteration 104, loss = 2.32271110\n",
      "Iteration 105, loss = 2.32238334\n",
      "Iteration 106, loss = 2.32147663\n",
      "Iteration 107, loss = 2.32045266\n",
      "Iteration 108, loss = 2.32107445\n",
      "Iteration 109, loss = 2.32048844\n",
      "Iteration 110, loss = 2.31990937\n",
      "Iteration 111, loss = 2.32085092\n",
      "Iteration 112, loss = 2.31986496\n",
      "Iteration 113, loss = 2.31955137\n",
      "Iteration 114, loss = 2.31929441\n",
      "Iteration 115, loss = 2.31848197\n",
      "Iteration 116, loss = 2.31801864\n",
      "Iteration 117, loss = 2.31848040\n",
      "Iteration 118, loss = 2.31762195\n",
      "Iteration 119, loss = 2.31842993\n",
      "Iteration 120, loss = 2.31709197\n",
      "Iteration 121, loss = 2.31601745\n",
      "Iteration 122, loss = 2.31679139\n",
      "Iteration 123, loss = 2.31686230\n",
      "Iteration 124, loss = 2.31528334\n",
      "Iteration 125, loss = 2.31458704\n",
      "Iteration 126, loss = 2.31477302\n",
      "Iteration 127, loss = 2.31280352\n",
      "Iteration 128, loss = 2.31473692\n",
      "Iteration 129, loss = 2.31516230\n",
      "Iteration 130, loss = 2.31690418\n",
      "Iteration 131, loss = 2.31357691\n",
      "Iteration 132, loss = 2.31445200\n",
      "Iteration 133, loss = 2.31282374\n",
      "Iteration 134, loss = 2.31252864\n",
      "Iteration 135, loss = 2.31280396\n",
      "Iteration 136, loss = 2.31220099\n",
      "Iteration 137, loss = 2.31228246\n",
      "Iteration 138, loss = 2.31017184\n",
      "Iteration 139, loss = 2.31293940\n",
      "Iteration 140, loss = 2.31106513\n",
      "Iteration 141, loss = 2.31072066\n",
      "Iteration 142, loss = 2.31141175\n",
      "Iteration 143, loss = 2.31040631\n",
      "Iteration 144, loss = 2.31190999\n",
      "Iteration 145, loss = 2.30993833\n",
      "Iteration 146, loss = 2.31017970\n",
      "Iteration 147, loss = 2.31026578\n",
      "Iteration 148, loss = 2.31009265\n",
      "Iteration 149, loss = 2.31001219\n",
      "Iteration 150, loss = 2.31003263\n",
      "Iteration 151, loss = 2.31068145\n",
      "Iteration 152, loss = 2.30818421\n",
      "Iteration 153, loss = 2.30921217\n",
      "Iteration 154, loss = 2.30845303\n",
      "Iteration 155, loss = 2.30879037\n",
      "Iteration 156, loss = 2.30871927\n",
      "Iteration 157, loss = 2.30760671\n",
      "Iteration 158, loss = 2.30677647\n",
      "Iteration 159, loss = 2.30929284\n",
      "Iteration 160, loss = 2.30848119\n",
      "Iteration 161, loss = 2.30657096\n",
      "Iteration 162, loss = 2.30850914\n",
      "Iteration 163, loss = 2.30865058\n",
      "Iteration 164, loss = 2.30633909\n",
      "Iteration 165, loss = 2.30536678\n",
      "Iteration 166, loss = 2.30754462\n",
      "Iteration 167, loss = 2.30725682\n",
      "Iteration 168, loss = 2.30711239\n",
      "Iteration 169, loss = 2.30736366\n",
      "Iteration 170, loss = 2.30629503\n",
      "Iteration 171, loss = 2.30476558\n",
      "Iteration 172, loss = 2.30476145\n",
      "Iteration 173, loss = 2.30620640\n",
      "Iteration 174, loss = 2.30538950\n",
      "Iteration 175, loss = 2.30349640\n",
      "Iteration 176, loss = 2.30355320\n",
      "Iteration 177, loss = 2.30546432\n",
      "Iteration 178, loss = 2.30429502\n",
      "Iteration 179, loss = 2.30517836\n",
      "Iteration 180, loss = 2.30591088\n",
      "Iteration 181, loss = 2.30455115\n",
      "Iteration 182, loss = 2.30445023\n",
      "Iteration 183, loss = 2.30277315\n",
      "Iteration 184, loss = 2.30393165\n",
      "Iteration 185, loss = 2.30397446\n",
      "Iteration 186, loss = 2.30181244\n",
      "Iteration 187, loss = 2.30395634\n",
      "Iteration 188, loss = 2.30290444\n",
      "Iteration 189, loss = 2.30336609\n",
      "Iteration 190, loss = 2.30319917\n",
      "Iteration 191, loss = 2.30276405\n",
      "Iteration 192, loss = 2.30226247\n",
      "Iteration 193, loss = 2.30422213\n",
      "Iteration 194, loss = 2.30249536\n",
      "Iteration 195, loss = 2.30199039\n",
      "Iteration 196, loss = 2.30133061\n",
      "Iteration 197, loss = 2.30296613\n",
      "Iteration 198, loss = 2.30154360\n",
      "Iteration 199, loss = 2.30084152\n",
      "Iteration 200, loss = 2.30200939\n",
      "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "             hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "             learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "             random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "             validation_fraction=0.1, verbose=True, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.142\n"
     ]
    }
   ],
   "source": [
    "regressors = [\n",
    "        RandomForestRegressor(max_depth=20, min_samples_leaf=50, n_estimators=20),\n",
    "        MLPRegressor(verbose=True)\n",
    "    ]\n",
    "\n",
    "for regressor in regressors:\n",
    "    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('regressor', regressor)])\n",
    "    pipe.fit(X_train, y_train)   \n",
    "    print(regressor)\n",
    "    print(\"model score: %.3f\" % pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# another gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV] regressor__max_depth=10, regressor__min_samples_leaf=50, regressor__n_estimators=10 \n",
      "[CV]  regressor__max_depth=10, regressor__min_samples_leaf=50, regressor__n_estimators=10, total=  11.4s\n",
      "[CV] regressor__max_depth=10, regressor__min_samples_leaf=50, regressor__n_estimators=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   11.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  regressor__max_depth=10, regressor__min_samples_leaf=50, regressor__n_estimators=10, total=  10.9s\n",
      "[CV] regressor__max_depth=10, regressor__min_samples_leaf=50, regressor__n_estimators=10 \n",
      "[CV]  regressor__max_depth=10, regressor__min_samples_leaf=50, regressor__n_estimators=10, total=  10.9s\n",
      "[CV] regressor__max_depth=10, regressor__min_samples_leaf=50, regressor__n_estimators=20 \n",
      "[CV]  regressor__max_depth=10, regressor__min_samples_leaf=50, regressor__n_estimators=20, total=  21.0s\n",
      "[CV] regressor__max_depth=10, regressor__min_samples_leaf=50, regressor__n_estimators=20 \n",
      "[CV]  regressor__max_depth=10, regressor__min_samples_leaf=50, regressor__n_estimators=20, total=  20.8s\n",
      "[CV] regressor__max_depth=10, regressor__min_samples_leaf=50, regressor__n_estimators=20 \n",
      "[CV]  regressor__max_depth=10, regressor__min_samples_leaf=50, regressor__n_estimators=20, total=  21.7s\n",
      "[CV] regressor__max_depth=10, regressor__min_samples_leaf=100, regressor__n_estimators=10 \n",
      "[CV]  regressor__max_depth=10, regressor__min_samples_leaf=100, regressor__n_estimators=10, total=  11.2s\n",
      "[CV] regressor__max_depth=10, regressor__min_samples_leaf=100, regressor__n_estimators=10 \n",
      "[CV]  regressor__max_depth=10, regressor__min_samples_leaf=100, regressor__n_estimators=10, total=  11.5s\n",
      "[CV] regressor__max_depth=10, regressor__min_samples_leaf=100, regressor__n_estimators=10 \n",
      "[CV]  regressor__max_depth=10, regressor__min_samples_leaf=100, regressor__n_estimators=10, total=  11.3s\n",
      "[CV] regressor__max_depth=10, regressor__min_samples_leaf=100, regressor__n_estimators=20 \n",
      "[CV]  regressor__max_depth=10, regressor__min_samples_leaf=100, regressor__n_estimators=20, total=  23.2s\n",
      "[CV] regressor__max_depth=10, regressor__min_samples_leaf=100, regressor__n_estimators=20 \n",
      "[CV]  regressor__max_depth=10, regressor__min_samples_leaf=100, regressor__n_estimators=20, total=  21.6s\n",
      "[CV] regressor__max_depth=10, regressor__min_samples_leaf=100, regressor__n_estimators=20 \n",
      "[CV]  regressor__max_depth=10, regressor__min_samples_leaf=100, regressor__n_estimators=20, total=  20.3s\n",
      "[CV] regressor__max_depth=20, regressor__min_samples_leaf=50, regressor__n_estimators=10 \n",
      "[CV]  regressor__max_depth=20, regressor__min_samples_leaf=50, regressor__n_estimators=10, total=  14.6s\n",
      "[CV] regressor__max_depth=20, regressor__min_samples_leaf=50, regressor__n_estimators=10 \n",
      "[CV]  regressor__max_depth=20, regressor__min_samples_leaf=50, regressor__n_estimators=10, total=  14.6s\n",
      "[CV] regressor__max_depth=20, regressor__min_samples_leaf=50, regressor__n_estimators=10 \n",
      "[CV]  regressor__max_depth=20, regressor__min_samples_leaf=50, regressor__n_estimators=10, total=  14.9s\n",
      "[CV] regressor__max_depth=20, regressor__min_samples_leaf=50, regressor__n_estimators=20 \n",
      "[CV]  regressor__max_depth=20, regressor__min_samples_leaf=50, regressor__n_estimators=20, total=  29.6s\n",
      "[CV] regressor__max_depth=20, regressor__min_samples_leaf=50, regressor__n_estimators=20 \n",
      "[CV]  regressor__max_depth=20, regressor__min_samples_leaf=50, regressor__n_estimators=20, total=  28.7s\n",
      "[CV] regressor__max_depth=20, regressor__min_samples_leaf=50, regressor__n_estimators=20 \n",
      "[CV]  regressor__max_depth=20, regressor__min_samples_leaf=50, regressor__n_estimators=20, total=  30.9s\n",
      "[CV] regressor__max_depth=20, regressor__min_samples_leaf=100, regressor__n_estimators=10 \n",
      "[CV]  regressor__max_depth=20, regressor__min_samples_leaf=100, regressor__n_estimators=10, total=  13.8s\n",
      "[CV] regressor__max_depth=20, regressor__min_samples_leaf=100, regressor__n_estimators=10 \n",
      "[CV]  regressor__max_depth=20, regressor__min_samples_leaf=100, regressor__n_estimators=10, total=  13.6s\n",
      "[CV] regressor__max_depth=20, regressor__min_samples_leaf=100, regressor__n_estimators=10 \n",
      "[CV]  regressor__max_depth=20, regressor__min_samples_leaf=100, regressor__n_estimators=10, total=  13.6s\n",
      "[CV] regressor__max_depth=20, regressor__min_samples_leaf=100, regressor__n_estimators=20 \n",
      "[CV]  regressor__max_depth=20, regressor__min_samples_leaf=100, regressor__n_estimators=20, total=  26.6s\n",
      "[CV] regressor__max_depth=20, regressor__min_samples_leaf=100, regressor__n_estimators=20 \n",
      "[CV]  regressor__max_depth=20, regressor__min_samples_leaf=100, regressor__n_estimators=20, total=  27.2s\n",
      "[CV] regressor__max_depth=20, regressor__min_samples_leaf=100, regressor__n_estimators=20 \n",
      "[CV]  regressor__max_depth=20, regressor__min_samples_leaf=100, regressor__n_estimators=20, total=  26.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:  7.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'regressor__max_depth': 20, 'regressor__min_samples_leaf': 50, 'regressor__n_estimators': 20}\n",
      "0.1469055615840664\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('regressor', RandomForestRegressor())])\n",
    "param_grid = { \n",
    "    'regressor__max_depth' : [10,20],\n",
    "    'regressor__min_samples_leaf': [50,100],\n",
    "    'regressor__n_estimators':[10,20]\n",
    "}\n",
    "\n",
    "CV = GridSearchCV(rf, param_grid, n_jobs=1,verbose=2)\n",
    "                  \n",
    "CV.fit(X, y)  \n",
    "print(CV.best_params_)    \n",
    "print(CV.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('num',\n",
       "                                                  StandardScaler(copy=True,\n",
       "                                                                 with_mean=True,\n",
       "                                                                 with_std=True),\n",
       "                                                  Index(['titleLength', 'logGoal', 'duration'], dtype='object')),\n",
       "                                                 ('cat',\n",
       "                                                  OneHotEncoder(cols=None,\n",
       "                                                                drop_invariant=False,\n",
       "                                                                handle_missing='value',\n",
       "                                                                ha...\n",
       "                                   verbose=False)),\n",
       "                ('regressor',\n",
       "                 RandomForestRegressor(bootstrap=True, criterion='mse',\n",
       "                                       max_depth=20, max_features='auto',\n",
       "                                       max_leaf_nodes=None,\n",
       "                                       min_impurity_decrease=0.0,\n",
       "                                       min_impurity_split=None,\n",
       "                                       min_samples_leaf=50, min_samples_split=2,\n",
       "                                       min_weight_fraction_leaf=0.0,\n",
       "                                       n_estimators=20, n_jobs=None,\n",
       "                                       oob_score=False, random_state=None,\n",
       "                                       verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('regressor', RandomForestRegressor(max_depth=20, min_samples_leaf=50, n_estimators=20))])\n",
    "pipe.fit(X_train, y_train)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = metrics.mean_absolute_error(y_test,y_pred)\n",
    "mse = metrics.mean_squared_error(y_test,y_pred)\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test,y_pred))\n",
    "r2 = metrics.r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae:  1.795077554986695\n",
      "mse:  4.982071590726378\n",
      "rmse:  2.2320554631832916\n",
      "r2:  0.14888980633058158\n"
     ]
    }
   ],
   "source": [
    "print(\"mae: \", mae)\n",
    "print(\"mse: \", mse)\n",
    "print(\"rmse: \", rmse)\n",
    "print(\"r2: \", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=list(zip(list(y_test), list(y_pred))),columns=['actual','predicted'])\n",
    "df['goal'] = list(X_test['logGoal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unlog the values\n",
    "for col in df.columns:\n",
    "    df[col] = df[col].apply(lambda x: 10**x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "      <th>goal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.851432e+09</td>\n",
       "      <td>1.849973e+07</td>\n",
       "      <td>2.466423e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.804166e+07</td>\n",
       "      <td>5.158715e+05</td>\n",
       "      <td>2.081312e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.218860e+08</td>\n",
       "      <td>1.949611e+06</td>\n",
       "      <td>7.973090e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.282213e+07</td>\n",
       "      <td>6.097497e+05</td>\n",
       "      <td>6.603562e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.879189e+08</td>\n",
       "      <td>1.948956e+07</td>\n",
       "      <td>4.097353e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59793</th>\n",
       "      <td>2.694816e+05</td>\n",
       "      <td>5.214472e+06</td>\n",
       "      <td>1.726996e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59794</th>\n",
       "      <td>1.572847e+07</td>\n",
       "      <td>1.557755e+07</td>\n",
       "      <td>3.289979e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59795</th>\n",
       "      <td>1.684897e+08</td>\n",
       "      <td>2.197475e+06</td>\n",
       "      <td>1.104713e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59796</th>\n",
       "      <td>2.151388e+05</td>\n",
       "      <td>1.452609e+06</td>\n",
       "      <td>7.185583e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59797</th>\n",
       "      <td>3.606754e+09</td>\n",
       "      <td>2.051756e+05</td>\n",
       "      <td>2.536663e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59798 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             actual     predicted          goal\n",
       "0      4.851432e+09  1.849973e+07  2.466423e+10\n",
       "1      2.804166e+07  5.158715e+05  2.081312e+06\n",
       "2      8.218860e+08  1.949611e+06  7.973090e+08\n",
       "3      6.282213e+07  6.097497e+05  6.603562e+10\n",
       "4      4.879189e+08  1.948956e+07  4.097353e+08\n",
       "...             ...           ...           ...\n",
       "59793  2.694816e+05  5.214472e+06  1.726996e+08\n",
       "59794  1.572847e+07  1.557755e+07  3.289979e+08\n",
       "59795  1.684897e+08  2.197475e+06  1.104713e+10\n",
       "59796  2.151388e+05  1.452609e+06  7.185583e+06\n",
       "59797  3.606754e+09  2.051756e+05  2.536663e+10\n",
       "\n",
       "[59798 rows x 3 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predictedPercent'] = df['predicted']/df['goal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['actualPercent'] = df['actual']/df['goal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so things stop showing up in scientific notation\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "      <th>goal</th>\n",
       "      <th>predictedPercent</th>\n",
       "      <th>actualPercent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4851432072.58310</td>\n",
       "      <td>18499733.91952</td>\n",
       "      <td>24664230324.55185</td>\n",
       "      <td>0.00075</td>\n",
       "      <td>0.19670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28041660.44956</td>\n",
       "      <td>515871.50677</td>\n",
       "      <td>2081312.46000</td>\n",
       "      <td>0.24786</td>\n",
       "      <td>13.47307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>821886022.24890</td>\n",
       "      <td>1949611.27230</td>\n",
       "      <td>797309020.00954</td>\n",
       "      <td>0.00245</td>\n",
       "      <td>1.03082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62822125.89978</td>\n",
       "      <td>609749.67432</td>\n",
       "      <td>66035623752.49438</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>487918884.40920</td>\n",
       "      <td>19489562.92835</td>\n",
       "      <td>409735290.38049</td>\n",
       "      <td>0.04757</td>\n",
       "      <td>1.19081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>282661748.62642</td>\n",
       "      <td>6949468.82761</td>\n",
       "      <td>66687741.70735</td>\n",
       "      <td>0.10421</td>\n",
       "      <td>4.23859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>68857129.82449</td>\n",
       "      <td>17763200.75069</td>\n",
       "      <td>66687741.70735</td>\n",
       "      <td>0.26636</td>\n",
       "      <td>1.03253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>717473710.75353</td>\n",
       "      <td>21755274.62277</td>\n",
       "      <td>196811123.30650</td>\n",
       "      <td>0.11054</td>\n",
       "      <td>3.64549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>173511964.90360</td>\n",
       "      <td>33513123.56080</td>\n",
       "      <td>144716565.64648</td>\n",
       "      <td>0.23158</td>\n",
       "      <td>1.19898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>399566882.67252</td>\n",
       "      <td>1730794.04102</td>\n",
       "      <td>8007326781.02461</td>\n",
       "      <td>0.00022</td>\n",
       "      <td>0.04990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>10245655.20154</td>\n",
       "      <td>13385392294.09025</td>\n",
       "      <td>0.00077</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>198754.68134</td>\n",
       "      <td>703661.24339</td>\n",
       "      <td>8086401.09376</td>\n",
       "      <td>0.08702</td>\n",
       "      <td>0.02458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2105285297.16650</td>\n",
       "      <td>40709301.46427</td>\n",
       "      <td>1513147151.03853</td>\n",
       "      <td>0.02690</td>\n",
       "      <td>1.39133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>990.22132</td>\n",
       "      <td>342321.64572</td>\n",
       "      <td>1639109.99452</td>\n",
       "      <td>0.20885</td>\n",
       "      <td>0.00060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>222386.04473</td>\n",
       "      <td>3596537.08558</td>\n",
       "      <td>66687741.70735</td>\n",
       "      <td>0.05393</td>\n",
       "      <td>0.00333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>724.29010</td>\n",
       "      <td>3100691.76164</td>\n",
       "      <td>18980299321.91020</td>\n",
       "      <td>0.00016</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>505571.37273</td>\n",
       "      <td>45731.92226</td>\n",
       "      <td>198754.68134</td>\n",
       "      <td>0.23009</td>\n",
       "      <td>2.54370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5652887.93780</td>\n",
       "      <td>1250936.73537</td>\n",
       "      <td>4837396.96361</td>\n",
       "      <td>0.25860</td>\n",
       "      <td>1.16858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>28949455.00396</td>\n",
       "      <td>20082.57135</td>\n",
       "      <td>322595079245174.81250</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8166.25628</td>\n",
       "      <td>517789.11310</td>\n",
       "      <td>376237516.95048</td>\n",
       "      <td>0.00138</td>\n",
       "      <td>0.00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6247492.95655</td>\n",
       "      <td>76489235.49259</td>\n",
       "      <td>20368200463.23745</td>\n",
       "      <td>0.00376</td>\n",
       "      <td>0.00031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>129487533.47474</td>\n",
       "      <td>11235830.18827</td>\n",
       "      <td>101476987.87547</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>1.27603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>27710.93990</td>\n",
       "      <td>144874.92277</td>\n",
       "      <td>39893529.33459</td>\n",
       "      <td>0.00363</td>\n",
       "      <td>0.00069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>365858481.10302</td>\n",
       "      <td>1808734.20994</td>\n",
       "      <td>15307349985.55965</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.02390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>40.68534</td>\n",
       "      <td>2072296.90457</td>\n",
       "      <td>144716565.64648</td>\n",
       "      <td>0.01432</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1079160.95006</td>\n",
       "      <td>204167.13376</td>\n",
       "      <td>456543.55449</td>\n",
       "      <td>0.44720</td>\n",
       "      <td>2.36376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>46899677.01016</td>\n",
       "      <td>985728.58974</td>\n",
       "      <td>282347855866.16919</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>40.68534</td>\n",
       "      <td>12612814.77740</td>\n",
       "      <td>970949898.47343</td>\n",
       "      <td>0.01299</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>16722287.25894</td>\n",
       "      <td>6812557.32192</td>\n",
       "      <td>101087978.19574</td>\n",
       "      <td>0.06739</td>\n",
       "      <td>0.16542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>226958469.22304</td>\n",
       "      <td>7289217.66405</td>\n",
       "      <td>196811123.30650</td>\n",
       "      <td>0.03704</td>\n",
       "      <td>1.15318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1577113141.12506</td>\n",
       "      <td>42385762.32412</td>\n",
       "      <td>8007326781.02461</td>\n",
       "      <td>0.00529</td>\n",
       "      <td>0.19696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>892570.54839</td>\n",
       "      <td>5033182.83713</td>\n",
       "      <td>360092357.86952</td>\n",
       "      <td>0.01398</td>\n",
       "      <td>0.00248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>160403.83534</td>\n",
       "      <td>1752226.82553</td>\n",
       "      <td>9567552333.29808</td>\n",
       "      <td>0.00018</td>\n",
       "      <td>0.00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>90352581.31564</td>\n",
       "      <td>5354809.40149</td>\n",
       "      <td>389779547.74895</td>\n",
       "      <td>0.01374</td>\n",
       "      <td>0.23180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>239826.60166</td>\n",
       "      <td>1040387.94076</td>\n",
       "      <td>2021392042.84890</td>\n",
       "      <td>0.00051</td>\n",
       "      <td>0.00012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>205686.59038</td>\n",
       "      <td>41453.50535</td>\n",
       "      <td>328997949.67042</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>0.00063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>209132.37304</td>\n",
       "      <td>39804289.63313</td>\n",
       "      <td>1021594888.30839</td>\n",
       "      <td>0.03896</td>\n",
       "      <td>0.00020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>56767507.75653</td>\n",
       "      <td>80478.96206</td>\n",
       "      <td>12304864.50342</td>\n",
       "      <td>0.00654</td>\n",
       "      <td>4.61342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>319594.69427</td>\n",
       "      <td>420679.54612</td>\n",
       "      <td>9425226.65493</td>\n",
       "      <td>0.04463</td>\n",
       "      <td>0.03391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>120377956.28538</td>\n",
       "      <td>3500143.18654</td>\n",
       "      <td>8086401.09376</td>\n",
       "      <td>0.43284</td>\n",
       "      <td>14.88647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>204137592.80656</td>\n",
       "      <td>24244159.75812</td>\n",
       "      <td>4128625399.93507</td>\n",
       "      <td>0.00587</td>\n",
       "      <td>0.04944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>358542793.55551</td>\n",
       "      <td>5454719.42511</td>\n",
       "      <td>6616347582.34612</td>\n",
       "      <td>0.00082</td>\n",
       "      <td>0.05419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>13400.91292</td>\n",
       "      <td>302101.21319</td>\n",
       "      <td>16670218169.96153</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>87426.36518</td>\n",
       "      <td>6491678.69886</td>\n",
       "      <td>196811123.30650</td>\n",
       "      <td>0.03298</td>\n",
       "      <td>0.00044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>274207.48717</td>\n",
       "      <td>6355264.35678</td>\n",
       "      <td>9972355046.83640</td>\n",
       "      <td>0.00064</td>\n",
       "      <td>0.00003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>952545.57726</td>\n",
       "      <td>2201930.11243</td>\n",
       "      <td>39503423355.65827</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3338583.98796</td>\n",
       "      <td>13410579.16769</td>\n",
       "      <td>4128625399.93507</td>\n",
       "      <td>0.00325</td>\n",
       "      <td>0.00081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>961.95220</td>\n",
       "      <td>3965099.97028</td>\n",
       "      <td>38757348.36983</td>\n",
       "      <td>0.10231</td>\n",
       "      <td>0.00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1655.29661</td>\n",
       "      <td>2935267.76873</td>\n",
       "      <td>328997949.67042</td>\n",
       "      <td>0.00892</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1418.66860</td>\n",
       "      <td>159284.38594</td>\n",
       "      <td>17145029388.54310</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2665967778.49619</td>\n",
       "      <td>27212611.52858</td>\n",
       "      <td>66035623752.49438</td>\n",
       "      <td>0.00041</td>\n",
       "      <td>0.04037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>274207.48717</td>\n",
       "      <td>314892.64709</td>\n",
       "      <td>198754.68134</td>\n",
       "      <td>1.58433</td>\n",
       "      <td>1.37963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>40287.48771</td>\n",
       "      <td>31726.55642</td>\n",
       "      <td>200.71743</td>\n",
       "      <td>158.06577</td>\n",
       "      <td>200.71743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>15475.85821</td>\n",
       "      <td>215918.49701</td>\n",
       "      <td>6344461.45647</td>\n",
       "      <td>0.03403</td>\n",
       "      <td>0.00244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>241278313.24668</td>\n",
       "      <td>15988029.38010</td>\n",
       "      <td>101476987.87547</td>\n",
       "      <td>0.15755</td>\n",
       "      <td>2.37767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1042.27635</td>\n",
       "      <td>1013353.77773</td>\n",
       "      <td>647355113.88382</td>\n",
       "      <td>0.00157</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>328997949.67042</td>\n",
       "      <td>3746613.10828</td>\n",
       "      <td>196811123.30650</td>\n",
       "      <td>0.01904</td>\n",
       "      <td>1.67164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>565682.36356</td>\n",
       "      <td>394203.63557</td>\n",
       "      <td>505571.37273</td>\n",
       "      <td>0.77972</td>\n",
       "      <td>1.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>31299827.48705</td>\n",
       "      <td>1492340.81403</td>\n",
       "      <td>20569341.43042</td>\n",
       "      <td>0.07255</td>\n",
       "      <td>1.52167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>27254117.25790</td>\n",
       "      <td>1149097.87866</td>\n",
       "      <td>101476987.87547</td>\n",
       "      <td>0.01132</td>\n",
       "      <td>0.26857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2908928004.71371</td>\n",
       "      <td>30059315.20659</td>\n",
       "      <td>1623081665.62811</td>\n",
       "      <td>0.01852</td>\n",
       "      <td>1.79223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1626531.61148</td>\n",
       "      <td>101088.60711</td>\n",
       "      <td>8007326781.02461</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>722665333.50295</td>\n",
       "      <td>8592758.18030</td>\n",
       "      <td>601945893.30481</td>\n",
       "      <td>0.01427</td>\n",
       "      <td>1.20055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>12426.37805</td>\n",
       "      <td>732425.54879</td>\n",
       "      <td>39893529.33459</td>\n",
       "      <td>0.01836</td>\n",
       "      <td>0.00031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>10845975.00575</td>\n",
       "      <td>492057.08530</td>\n",
       "      <td>1116404150.66706</td>\n",
       "      <td>0.00044</td>\n",
       "      <td>0.00972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>669884.77505</td>\n",
       "      <td>3787691.95516</td>\n",
       "      <td>167974513728.24033</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1427.06569</td>\n",
       "      <td>6556107.40432</td>\n",
       "      <td>4534708672.99667</td>\n",
       "      <td>0.00145</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1121867447.10359</td>\n",
       "      <td>12384643.20366</td>\n",
       "      <td>825234128.23066</td>\n",
       "      <td>0.01501</td>\n",
       "      <td>1.35945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>10519047.66591</td>\n",
       "      <td>284065.13915</td>\n",
       "      <td>8086401.09376</td>\n",
       "      <td>0.03513</td>\n",
       "      <td>1.30083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>4768064.42274</td>\n",
       "      <td>5545017.22442</td>\n",
       "      <td>101476987.87547</td>\n",
       "      <td>0.05464</td>\n",
       "      <td>0.04699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>52299.66069</td>\n",
       "      <td>19029.61650</td>\n",
       "      <td>13254500852511.31641</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>381032518.61765</td>\n",
       "      <td>412093.97699</td>\n",
       "      <td>23729297686.20925</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.01606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>12.16716</td>\n",
       "      <td>23537.78806</td>\n",
       "      <td>3924295272579120.50000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>720996.49667</td>\n",
       "      <td>341899.48992</td>\n",
       "      <td>328997949.67042</td>\n",
       "      <td>0.00104</td>\n",
       "      <td>0.00219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>8166.25628</td>\n",
       "      <td>969488.84391</td>\n",
       "      <td>144716565.64648</td>\n",
       "      <td>0.00670</td>\n",
       "      <td>0.00006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>332246.88499</td>\n",
       "      <td>1258033.04504</td>\n",
       "      <td>13385392294.09025</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>3265.35687</td>\n",
       "      <td>46045.09073</td>\n",
       "      <td>10005813.75897</td>\n",
       "      <td>0.00460</td>\n",
       "      <td>0.00033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1939402774.49585</td>\n",
       "      <td>890149.03589</td>\n",
       "      <td>1021594888.30839</td>\n",
       "      <td>0.00087</td>\n",
       "      <td>1.89841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>7593071211.60260</td>\n",
       "      <td>253761692.46464</td>\n",
       "      <td>5507679030.20087</td>\n",
       "      <td>0.04607</td>\n",
       "      <td>1.37863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>850333.21488</td>\n",
       "      <td>1291557.88518</td>\n",
       "      <td>66687741.70735</td>\n",
       "      <td>0.01937</td>\n",
       "      <td>0.01275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1976.22717</td>\n",
       "      <td>2833703.21967</td>\n",
       "      <td>1273444014.07755</td>\n",
       "      <td>0.00223</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>4.93341</td>\n",
       "      <td>4072099.20429</td>\n",
       "      <td>500627553.05571</td>\n",
       "      <td>0.00813</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>848995546.84144</td>\n",
       "      <td>15386442.91134</td>\n",
       "      <td>836870577.92634</td>\n",
       "      <td>0.01839</td>\n",
       "      <td>1.01449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>35006.24147</td>\n",
       "      <td>28380.08120</td>\n",
       "      <td>24546.79549</td>\n",
       "      <td>1.15616</td>\n",
       "      <td>1.42610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>510.56401</td>\n",
       "      <td>11363109.76661</td>\n",
       "      <td>328997949.67042</td>\n",
       "      <td>0.03454</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2946119.21612</td>\n",
       "      <td>153931.65801</td>\n",
       "      <td>505571.37273</td>\n",
       "      <td>0.30447</td>\n",
       "      <td>5.82731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>48341939.18535</td>\n",
       "      <td>23569225.05435</td>\n",
       "      <td>8086401.09376</td>\n",
       "      <td>2.91467</td>\n",
       "      <td>5.97818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>47079.17399</td>\n",
       "      <td>32491.72967</td>\n",
       "      <td>40287.48771</td>\n",
       "      <td>0.80650</td>\n",
       "      <td>1.16858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>87426.36518</td>\n",
       "      <td>3113529.16972</td>\n",
       "      <td>101476987.87547</td>\n",
       "      <td>0.03068</td>\n",
       "      <td>0.00086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>332246.88499</td>\n",
       "      <td>39165322.76532</td>\n",
       "      <td>836870577.92634</td>\n",
       "      <td>0.04680</td>\n",
       "      <td>0.00040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>26446834.71305</td>\n",
       "      <td>170796.14094</td>\n",
       "      <td>20569341.43042</td>\n",
       "      <td>0.00830</td>\n",
       "      <td>1.28574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>23842543.06199</td>\n",
       "      <td>23123104.04602</td>\n",
       "      <td>1559281742.80762</td>\n",
       "      <td>0.01483</td>\n",
       "      <td>0.01529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>296759758.25494</td>\n",
       "      <td>4224580.41933</td>\n",
       "      <td>174886523.92071</td>\n",
       "      <td>0.02416</td>\n",
       "      <td>1.69687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>2518.82144</td>\n",
       "      <td>16135702.39400</td>\n",
       "      <td>1623081665.62811</td>\n",
       "      <td>0.00994</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>18964376.68163</td>\n",
       "      <td>58480.47866</td>\n",
       "      <td>198754.68134</td>\n",
       "      <td>0.29423</td>\n",
       "      <td>95.41600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>13832.79788</td>\n",
       "      <td>10784740.09786</td>\n",
       "      <td>106712146.77363</td>\n",
       "      <td>0.10106</td>\n",
       "      <td>0.00013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>990.22132</td>\n",
       "      <td>946345.97740</td>\n",
       "      <td>1639109.99452</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.00060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>281550655.43999</td>\n",
       "      <td>32393837.31225</td>\n",
       "      <td>144716565.64648</td>\n",
       "      <td>0.22384</td>\n",
       "      <td>1.94553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>379022082.52701</td>\n",
       "      <td>862870.23936</td>\n",
       "      <td>66687741.70735</td>\n",
       "      <td>0.01294</td>\n",
       "      <td>5.68353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>55581.73872</td>\n",
       "      <td>185641.11266</td>\n",
       "      <td>1639109.99452</td>\n",
       "      <td>0.11326</td>\n",
       "      <td>0.03391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             actual       predicted                   goal  predictedPercent  \\\n",
       "0  4851432072.58310  18499733.91952      24664230324.55185           0.00075   \n",
       "1    28041660.44956    515871.50677          2081312.46000           0.24786   \n",
       "2   821886022.24890   1949611.27230        797309020.00954           0.00245   \n",
       "3    62822125.89978    609749.67432      66035623752.49438           0.00001   \n",
       "4   487918884.40920  19489562.92835        409735290.38049           0.04757   \n",
       "5   282661748.62642   6949468.82761         66687741.70735           0.10421   \n",
       "6    68857129.82449  17763200.75069         66687741.70735           0.26636   \n",
       "7   717473710.75353  21755274.62277        196811123.30650           0.11054   \n",
       "8   173511964.90360  33513123.56080        144716565.64648           0.23158   \n",
       "9   399566882.67252   1730794.04102       8007326781.02461           0.00022   \n",
       "10          1.00000  10245655.20154      13385392294.09025           0.00077   \n",
       "11     198754.68134    703661.24339          8086401.09376           0.08702   \n",
       "12 2105285297.16650  40709301.46427       1513147151.03853           0.02690   \n",
       "13        990.22132    342321.64572          1639109.99452           0.20885   \n",
       "14     222386.04473   3596537.08558         66687741.70735           0.05393   \n",
       "15        724.29010   3100691.76164      18980299321.91020           0.00016   \n",
       "16     505571.37273     45731.92226           198754.68134           0.23009   \n",
       "17    5652887.93780   1250936.73537          4837396.96361           0.25860   \n",
       "18   28949455.00396     20082.57135  322595079245174.81250           0.00000   \n",
       "19       8166.25628    517789.11310        376237516.95048           0.00138   \n",
       "20    6247492.95655  76489235.49259      20368200463.23745           0.00376   \n",
       "21  129487533.47474  11235830.18827        101476987.87547           0.11072   \n",
       "22      27710.93990    144874.92277         39893529.33459           0.00363   \n",
       "23  365858481.10302   1808734.20994      15307349985.55965           0.00012   \n",
       "24         40.68534   2072296.90457        144716565.64648           0.01432   \n",
       "25    1079160.95006    204167.13376           456543.55449           0.44720   \n",
       "26   46899677.01016    985728.58974     282347855866.16919           0.00000   \n",
       "27         40.68534  12612814.77740        970949898.47343           0.01299   \n",
       "28   16722287.25894   6812557.32192        101087978.19574           0.06739   \n",
       "29  226958469.22304   7289217.66405        196811123.30650           0.03704   \n",
       "30 1577113141.12506  42385762.32412       8007326781.02461           0.00529   \n",
       "31     892570.54839   5033182.83713        360092357.86952           0.01398   \n",
       "32     160403.83534   1752226.82553       9567552333.29808           0.00018   \n",
       "33   90352581.31564   5354809.40149        389779547.74895           0.01374   \n",
       "34     239826.60166   1040387.94076       2021392042.84890           0.00051   \n",
       "35     205686.59038     41453.50535        328997949.67042           0.00013   \n",
       "36     209132.37304  39804289.63313       1021594888.30839           0.03896   \n",
       "37   56767507.75653     80478.96206         12304864.50342           0.00654   \n",
       "38     319594.69427    420679.54612          9425226.65493           0.04463   \n",
       "39  120377956.28538   3500143.18654          8086401.09376           0.43284   \n",
       "40  204137592.80656  24244159.75812       4128625399.93507           0.00587   \n",
       "41  358542793.55551   5454719.42511       6616347582.34612           0.00082   \n",
       "42      13400.91292    302101.21319      16670218169.96153           0.00002   \n",
       "43      87426.36518   6491678.69886        196811123.30650           0.03298   \n",
       "44     274207.48717   6355264.35678       9972355046.83640           0.00064   \n",
       "45     952545.57726   2201930.11243      39503423355.65827           0.00006   \n",
       "46    3338583.98796  13410579.16769       4128625399.93507           0.00325   \n",
       "47        961.95220   3965099.97028         38757348.36983           0.10231   \n",
       "48       1655.29661   2935267.76873        328997949.67042           0.00892   \n",
       "49       1418.66860    159284.38594      17145029388.54310           0.00001   \n",
       "50 2665967778.49619  27212611.52858      66035623752.49438           0.00041   \n",
       "51     274207.48717    314892.64709           198754.68134           1.58433   \n",
       "52      40287.48771     31726.55642              200.71743         158.06577   \n",
       "53      15475.85821    215918.49701          6344461.45647           0.03403   \n",
       "54  241278313.24668  15988029.38010        101476987.87547           0.15755   \n",
       "55       1042.27635   1013353.77773        647355113.88382           0.00157   \n",
       "56  328997949.67042   3746613.10828        196811123.30650           0.01904   \n",
       "57     565682.36356    394203.63557           505571.37273           0.77972   \n",
       "58   31299827.48705   1492340.81403         20569341.43042           0.07255   \n",
       "59   27254117.25790   1149097.87866        101476987.87547           0.01132   \n",
       "60 2908928004.71371  30059315.20659       1623081665.62811           0.01852   \n",
       "61    1626531.61148    101088.60711       8007326781.02461           0.00001   \n",
       "62  722665333.50295   8592758.18030        601945893.30481           0.01427   \n",
       "63      12426.37805    732425.54879         39893529.33459           0.01836   \n",
       "64   10845975.00575    492057.08530       1116404150.66706           0.00044   \n",
       "65     669884.77505   3787691.95516     167974513728.24033           0.00002   \n",
       "66       1427.06569   6556107.40432       4534708672.99667           0.00145   \n",
       "67 1121867447.10359  12384643.20366        825234128.23066           0.01501   \n",
       "68   10519047.66591    284065.13915          8086401.09376           0.03513   \n",
       "69    4768064.42274   5545017.22442        101476987.87547           0.05464   \n",
       "70      52299.66069     19029.61650   13254500852511.31641           0.00000   \n",
       "71  381032518.61765    412093.97699      23729297686.20925           0.00002   \n",
       "72         12.16716     23537.78806 3924295272579120.50000           0.00000   \n",
       "73     720996.49667    341899.48992        328997949.67042           0.00104   \n",
       "74       8166.25628    969488.84391        144716565.64648           0.00670   \n",
       "75     332246.88499   1258033.04504      13385392294.09025           0.00009   \n",
       "76       3265.35687     46045.09073         10005813.75897           0.00460   \n",
       "77 1939402774.49585    890149.03589       1021594888.30839           0.00087   \n",
       "78 7593071211.60260 253761692.46464       5507679030.20087           0.04607   \n",
       "79     850333.21488   1291557.88518         66687741.70735           0.01937   \n",
       "80       1976.22717   2833703.21967       1273444014.07755           0.00223   \n",
       "81          4.93341   4072099.20429        500627553.05571           0.00813   \n",
       "82  848995546.84144  15386442.91134        836870577.92634           0.01839   \n",
       "83      35006.24147     28380.08120            24546.79549           1.15616   \n",
       "84        510.56401  11363109.76661        328997949.67042           0.03454   \n",
       "85    2946119.21612    153931.65801           505571.37273           0.30447   \n",
       "86   48341939.18535  23569225.05435          8086401.09376           2.91467   \n",
       "87      47079.17399     32491.72967            40287.48771           0.80650   \n",
       "88      87426.36518   3113529.16972        101476987.87547           0.03068   \n",
       "89     332246.88499  39165322.76532        836870577.92634           0.04680   \n",
       "90   26446834.71305    170796.14094         20569341.43042           0.00830   \n",
       "91   23842543.06199  23123104.04602       1559281742.80762           0.01483   \n",
       "92  296759758.25494   4224580.41933        174886523.92071           0.02416   \n",
       "93       2518.82144  16135702.39400       1623081665.62811           0.00994   \n",
       "94   18964376.68163     58480.47866           198754.68134           0.29423   \n",
       "95      13832.79788  10784740.09786        106712146.77363           0.10106   \n",
       "96        990.22132    946345.97740          1639109.99452           0.57735   \n",
       "97  281550655.43999  32393837.31225        144716565.64648           0.22384   \n",
       "98  379022082.52701    862870.23936         66687741.70735           0.01294   \n",
       "99      55581.73872    185641.11266          1639109.99452           0.11326   \n",
       "\n",
       "    actualPercent  \n",
       "0         0.19670  \n",
       "1        13.47307  \n",
       "2         1.03082  \n",
       "3         0.00095  \n",
       "4         1.19081  \n",
       "5         4.23859  \n",
       "6         1.03253  \n",
       "7         3.64549  \n",
       "8         1.19898  \n",
       "9         0.04990  \n",
       "10        0.00000  \n",
       "11        0.02458  \n",
       "12        1.39133  \n",
       "13        0.00060  \n",
       "14        0.00333  \n",
       "15        0.00000  \n",
       "16        2.54370  \n",
       "17        1.16858  \n",
       "18        0.00000  \n",
       "19        0.00002  \n",
       "20        0.00031  \n",
       "21        1.27603  \n",
       "22        0.00069  \n",
       "23        0.02390  \n",
       "24        0.00000  \n",
       "25        2.36376  \n",
       "26        0.00017  \n",
       "27        0.00000  \n",
       "28        0.16542  \n",
       "29        1.15318  \n",
       "30        0.19696  \n",
       "31        0.00248  \n",
       "32        0.00002  \n",
       "33        0.23180  \n",
       "34        0.00012  \n",
       "35        0.00063  \n",
       "36        0.00020  \n",
       "37        4.61342  \n",
       "38        0.03391  \n",
       "39       14.88647  \n",
       "40        0.04944  \n",
       "41        0.05419  \n",
       "42        0.00000  \n",
       "43        0.00044  \n",
       "44        0.00003  \n",
       "45        0.00002  \n",
       "46        0.00081  \n",
       "47        0.00002  \n",
       "48        0.00001  \n",
       "49        0.00000  \n",
       "50        0.04037  \n",
       "51        1.37963  \n",
       "52      200.71743  \n",
       "53        0.00244  \n",
       "54        2.37767  \n",
       "55        0.00000  \n",
       "56        1.67164  \n",
       "57        1.11890  \n",
       "58        1.52167  \n",
       "59        0.26857  \n",
       "60        1.79223  \n",
       "61        0.00020  \n",
       "62        1.20055  \n",
       "63        0.00031  \n",
       "64        0.00972  \n",
       "65        0.00000  \n",
       "66        0.00000  \n",
       "67        1.35945  \n",
       "68        1.30083  \n",
       "69        0.04699  \n",
       "70        0.00000  \n",
       "71        0.01606  \n",
       "72        0.00000  \n",
       "73        0.00219  \n",
       "74        0.00006  \n",
       "75        0.00002  \n",
       "76        0.00033  \n",
       "77        1.89841  \n",
       "78        1.37863  \n",
       "79        0.01275  \n",
       "80        0.00000  \n",
       "81        0.00000  \n",
       "82        1.01449  \n",
       "83        1.42610  \n",
       "84        0.00000  \n",
       "85        5.82731  \n",
       "86        5.97818  \n",
       "87        1.16858  \n",
       "88        0.00086  \n",
       "89        0.00040  \n",
       "90        1.28574  \n",
       "91        0.01529  \n",
       "92        1.69687  \n",
       "93        0.00000  \n",
       "94       95.41600  \n",
       "95        0.00013  \n",
       "96        0.00060  \n",
       "97        1.94553  \n",
       "98        5.68353  \n",
       "99        0.03391  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "      <th>goal</th>\n",
       "      <th>predictedPercent</th>\n",
       "      <th>actualPercent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53244</th>\n",
       "      <td>5114386817.14458</td>\n",
       "      <td>53110.58859</td>\n",
       "      <td>0.93227</td>\n",
       "      <td>56969.22638</td>\n",
       "      <td>5485961803.60920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31464</th>\n",
       "      <td>4656057677.27076</td>\n",
       "      <td>53433.49888</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>53433.49888</td>\n",
       "      <td>4656057677.27076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7137</th>\n",
       "      <td>2219730826.20441</td>\n",
       "      <td>100811.93095</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>100811.93095</td>\n",
       "      <td>2219730826.20441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36886</th>\n",
       "      <td>40287.48771</td>\n",
       "      <td>348470.56416</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>14039003569.28536</td>\n",
       "      <td>1623081665.62811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46169</th>\n",
       "      <td>428322704.56447</td>\n",
       "      <td>61470.55846</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>61470.55846</td>\n",
       "      <td>428322704.56447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2841</th>\n",
       "      <td>4.93341</td>\n",
       "      <td>4987.61405</td>\n",
       "      <td>2634394093298117120.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27568</th>\n",
       "      <td>1.27163</td>\n",
       "      <td>29006.77916</td>\n",
       "      <td>2002313035480175616.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6574</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>26678.58322</td>\n",
       "      <td>2634394093298117120.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57780</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>3337210.92225</td>\n",
       "      <td>2634394093298117120.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25208</th>\n",
       "      <td>0.72567</td>\n",
       "      <td>5693.83347</td>\n",
       "      <td>1916395953725111552.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59798 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                actual     predicted                      goal  \\\n",
       "53244 5114386817.14458   53110.58859                   0.93227   \n",
       "31464 4656057677.27076   53433.49888                   1.00000   \n",
       "7137  2219730826.20441  100811.93095                   1.00000   \n",
       "36886      40287.48771  348470.56416                   0.00002   \n",
       "46169  428322704.56447   61470.55846                   1.00000   \n",
       "...                ...           ...                       ...   \n",
       "2841           4.93341    4987.61405 2634394093298117120.00000   \n",
       "27568          1.27163   29006.77916 2002313035480175616.00000   \n",
       "6574           1.00000   26678.58322 2634394093298117120.00000   \n",
       "57780          1.00000 3337210.92225 2634394093298117120.00000   \n",
       "25208          0.72567    5693.83347 1916395953725111552.00000   \n",
       "\n",
       "       predictedPercent    actualPercent  \n",
       "53244       56969.22638 5485961803.60920  \n",
       "31464       53433.49888 4656057677.27076  \n",
       "7137       100811.93095 2219730826.20441  \n",
       "36886 14039003569.28536 1623081665.62811  \n",
       "46169       61470.55846  428322704.56447  \n",
       "...                 ...              ...  \n",
       "2841            0.00000          0.00000  \n",
       "27568           0.00000          0.00000  \n",
       "6574            0.00000          0.00000  \n",
       "57780           0.00000          0.00000  \n",
       "25208           0.00000          0.00000  \n",
       "\n",
       "[59798 rows x 5 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=\"actualPercent\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = metrics.mean_absolute_error(df['actualPercent'],df['predictedPercent'])\n",
    "mse = metrics.mean_squared_error(df['actualPercent'],df['predictedPercent'])\n",
    "rmse = np.sqrt(metrics.mean_squared_error(df['actualPercent'],df['predictedPercent']))\n",
    "r2 = metrics.r2_score(df['actualPercent'], df['predictedPercent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae:  444113.25714209676\n",
      "mse:  3532685261192755.0\n",
      "rmse:  59436396.77161423\n",
      "r2:  -2.53700867884389\n"
     ]
    }
   ],
   "source": [
    "print(\"mae: \", mae)\n",
    "print(\"mse: \", mse)\n",
    "print(\"rmse: \", rmse)\n",
    "print(\"r2: \", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
